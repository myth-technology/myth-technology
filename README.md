
---

### ðŸ§­ Mission
We build practical AI systems that ship: **robust agents, reliable evaluation, and efficient inference**.  
Our open-source work focuses on **developer experience** and **operational reliability** so teams can go from idea â†’ prototype â†’ production with confidence.

---

### ðŸ”® What weâ€™re working on
- **myth/agent-kit** â€“ Composable agent runtime (tools, memory, routing) with predictable behavior.
- **myth/eval** â€“ Task-driven evaluation: datasets, metrics, judges, and report cards.
- **myth/serve** â€“ Minimal, fast inference gateway with caching, cost tracking, and fallbacks.
- **myth/prompt** â€“ Typed prompt pipelines, versioning, and sandboxed tests.
- **myth/datasets** â€“ Curated, reproducible corpora for agent/eval workflows.

> Tip: Pin your top 6 repos on the org to mirror the list above.

---

### ðŸ—‚ Featured
| Repo | What it is | Try it |
|---|---|---|
| [`agent-kit`](https://github.com/myth-ai/agent-kit) | Agent runtime with tools, memory, retries, and tracing. | `npm i @mythai/agent-kit` |
| [`eval`](https://github.com/myth-ai/eval) | CLI + SDK for LLM task evaluation with model judges. | `pip install myth-eval` |
| [`serve`](https://github.com/myth-ai/serve) | Bring-your-own-LLM gateway with smart routing & caches. | Docker / Helm charts |
| [`prompt`](https://github.com/myth-ai/prompt) | Prompt pipelines with schema-checked I/O and tests. | TS/JS SDK |
| [`datasets`](https://github.com)

